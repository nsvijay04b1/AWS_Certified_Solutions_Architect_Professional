# Amazon S3 (Simple Storage Service):
- Object Storage service via HTTP/S.
- You cannot modify an object in S3: you need to overwrite it.
- Consistency:
	- new objects: strong read-after-write consistency.
	- replaced objects: eventual consistency.
- Buckets:
	- A bucket is a container for objects. 
	- An S3 bucket name is globally unique across all AWS accounts. 
	- A Bucket resides in only one region.
- Objects:
	- An object is a file with any metadata that describes that file.
	- An object key is the unique identifier for an object within a bucket. 
	- The combination of a bucket, key, and version ID uniquely identify each object.
	- Every object in Amazon S3 can be uniquely addressed through the combination of the web service endpoint, bucket name, key, and optionally, a version. 
- HA: S3 data replicated across 3 AZs at least (except the One Zone-IA and RRS classes).
- Block Public Access option: to ensure that public access to all your S3 buckets and objects is blocked. This setting applies account-wide for all current and future buckets. 
- Supports events notifications. Targets: SNS, SQS & Lambda.
- Object Versioning option:
	- If disabled, S3 sets the value of the version ID to "null". 
	- If enabled, S3 automatically generates a unique version ID for every new object that is created or updated. 
	- Object versions that existed before you enable the versioning option keep their version ID as "null". 

# S3 Storage Classes (per object):
- S3 Standard: low latency, high performance, high cost. Default class. No retrieval fees.
- S3 Reduced Redundancy (RRS): low availability (not recommended)
- S3 Standard-IA: Infrequent Access, same performance as S3-Standard, medium cost, min storage 30 days, retrieval fee.
- S3 One Zone-IA: Infrequent Access, stored only in one AZ 
- S3 Glacier: Archival, Retrieval fee, min storage 90 days, 3 retrieval speeds (from minutes to 12 hours).
- S3 Glacier Deep Archive: Archival, Retrieval fee, min storage 180 days, 2 retrieval speeds (12 or 24 hours). Internally uses S3 Glacier.
- S3 Intelligent-Tiering: automatically moving data to the most cost-effective access tier, without operational overhead. Min 30 days. No retrieval fees.

# S3 Object size:
- The total volume of data and number of objects you can store are unlimited. 
- Max size of an object is 5 TB.
- Max object upload in a single PUT is 5 GB. Beyond this, you need to use multipart upload. 

# S3 vs S3 Glacier naming:
- File Container: 	S3=Bucket, Glacier=Vault.
- File: 			S3=Object, Glacier=Archive.

# S3 Bucket options:
- Versioning.
- Cross Region Replication (CRR). Requires to activate versioning.
- Data lifecycle management.
- MFA delete
- Requester pays for the data transfers.
- Notification.

# S3 Access security:
- You can use Bucket policies (IAM resource policies) or ACLs.
- ACLs:
	- Each bucket and object has an ACL attached to it as a subresource.
	- An ACL defines which AWS accounts or groups are granted access and the type of access. 
	- ACLs grant basic read/write permissions to other AWS accounts. 
	- You can grant permissions only to other AWS accounts, but not to users in the same account.
	- ACLs are very limited options. It's better to use Bucket policies.
- Main use cases for ACLs:
	- An object ACL is the only way to manage access to objects that are not owned by the bucket owner.
	- Manage permissions at the object level for a large number of objects (Policies are limited in size).
	- Use bucket ACL to grant write permission to the Amazon S3 Log Delivery group to write access log objects to your bucket.


# S3 Bucket access URL:
- Direct Access: Amazon S3 supports two URL styles to access a bucket directly:
	- virtual-hosted-style: https://{bucket-name}.s3.{Region}.amazonaws.com/{key-name}
	- path-style: https://s3.{Region}.amazonaws.com/{bucket-name}/{key-name}
- S3 Access Point: 
	- Access points are named network endpoints that are attached to buckets.
	- Each access point enforces distinct permissions and network controls.
	- You can create multiple access points for a single bucket to manage access policies in different use cases instead of a single bucket policy that spans dozens or hundreds of use cases.
	- supports only virtual-host-style addressing.
	- Can be either accessed from Internet ("NetworkOrigin": "Internet") or from a VPC ("NetworkOrigin": "VPC" and you specify the VPC ID).
- S3://
	- Some AWS services require specifying an Amazon S3 bucket using the S3:// notation.
	- Format: s3://{bucket-name}/{key-name}

# S3 Presigned URLs:
- A presigned URL gives you access to the object identified in the URL for specified duration.
- When you create a presigned URL, you associate it with a specific action.
- Anyone using the URL will act on the object as if he were the original signing user. 

# S3 Replication:
- Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets.
- Single or multiple destinations.
- Same or different accounts.
- Same or different region (SRR or CRR).
- Same or different storage class.
- Replication Time Control (RTC) feature enables SLA of 15 minutes for replication of 99.9% of objects during any billing month.
- Read access available for replicas.

# Prefixes and S3 Performance:
- A prefix is a logical grouping of the objects in a bucket.
- The prefix value is similar to a directory name that enables you to store similar data under the same directory in a bucket.
- Can create a hierarchy. 
- You can chose your delimiter for the bucket, such as slash (/).
- S3 Request rates: 3,500 writes and 5,500 reads per second PER PREFIX!
- To parallelize reads/writes, use multiple prefixes.

# S3 Transfer Acceleration: 
- A bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. 
- Takes advantage of the globally distributed edge locations in Amazon CloudFront. 
- As the data arrives at an edge location, the data is routed to Amazon S3 over an optimized network path. 
- Popular use cases:
	- Your customers upload to a centralized bucket from all over the world.
	- You transfer gigabytes to terabytes of data on a regular basis across continents.
	- You can't use all of your available bandwidth over the internet when uploading to Amazon S3.
- To access the bucket that is enabled for Transfer Acceleration, you must use the endpoint {bucket-name}.s3-accelerate.amazonaws.com. 
- You can use the Amazon S3 Transfer Acceleration Speed Comparison tool to compare accelerated and non-accelerated upload speeds across Amazon S3 Regions.

# S3 Encryption - Server-Side Encryption (SSE):
You have three mutually exclusive options, depending on how you choose to manage the encryption keys:
- Server-Side Encryption with S3-Managed Keys (SSE-S3).
- Server-Side Encryption with CMKs stored in KMS (SSE-KMS).
- Server-Side Encryption with Customer-Provided Keys (SSE-C).

### SSE-S3:
- Each object is encrypted with a unique key.
- Encrypts the key itself with a master key that it regularly rotates. 
- Uses AES-256.
- Users do not manage the key or key permission.

### SSE-KMS:
- S3 uses AWS KMS customer master keys (CMKs) to encrypt your Amazon S3 objects. 
- Encrypts only the object data. Any object metadata is not encrypted. 
- When you use SSE-KMS encryption with an S3 bucket, the AWS KMS CMK must be in the same Region as the bucket. 
- You can use the default AWS-managed CMK, or you can specify a customer-managed CMK that you have already created. 
- Users who have access to a bucket will not see the encrypted files if they do not have permissions to use the encryption key in KMS. So you need to manage the key permissions in KMS even if the key is an AWS-managed CMK.

### SSE-C:
- you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. 
- Uses AES-256.
- The encryption key must be provided in every request.

Hosting a static website on S3:
- You can configure your bucket as a static website.
- Website endpoints are different from the endpoints where you send REST API requests. 
- You configure the index document for the website.
- HTTPS not supported.
- When using your own domain name, bucket names must match your domain name exactly.

# CORS in S3:
- About the CORS standard in general:
	- The same-origin policy is a browser security policy that restricts on-page scripts from accessing or posting data to resources from a different host, on a different port, or with a different protocol than the current page. These resources are called "Cross-Origin Resources".
	- The HTML document itself can access cross-origin resources. However, scripts cannot access cross-origin resources. This is because a script could send data while accessing a cross-origin resource.
	- CORS, or Cross Origin Resource Sharing, is a method of accessing cross-origin resources safely. 
	- With CORS, the browser first does a preflight OPTIONS request on the target server to check if the server accepts CORS. Then the browser sends the actual request with proper CORS headers. 
- For Web pages hosted on S3, you need to enable the CORS option on the target bucket that is referenced by your web page.

# Query in Place options in S3:
- S3 Select:
	- you can use simple SQL statements to filter contents of S3 objects and retrieve just the subset of data that you need. 
	- Works on objects stored in CSV, JSON, or Apache Parquet format.
	- Supports compressed objects in GZIP and BZIP2 for CSV and JSON.
	- supports server-side encrypted objects.
- Amazon Athena.
- Amazon Redshift Spectrum.

Query in Place options in S3 Glacier:
- S3 Glacier Select:
	- you can use simple SQL statements to filter contents of S3 objects and retrieve just the subset of data that you need. 
	- retrieved data is stored in S3.
	- Works only on uncompressed CSV files.
	- You need to create a Glacier Job to run the SQL request.

# S3 Batch Operations:
- A feature that you can use to automate the execution, management, and auditing of a specific S3 request or Lambda function across many objects stored in Amazon S3. 
- You can use S3 Batch Operations to automate replacing tag sets on S3 objects, updating access control lists (ACL) for S3 objects, copying storage between buckets, initiating a restore from Glacier to S3, or performing custom operations with Lambda functions.
- You should use S3 Batch Operations if you want to automate the execution of a single operation (like copying an object, or executing an AWS Lambda function) across many objects. 
- With S3 Batch Operations, you can, with a few clicks in the S3 console or a single API request, make a change to billions of objects without having to write custom application code or run compute clusters for storage management applications. 

# S3 Object Lock:
- Blocks object version deletion during a customer-defined retention period so that you can enforce retention policies as an added layer of data protection or for regulatory compliance.
- If a user attempts to delete an object before its "Retain Until Date" has passed, the operation will be denied.
- Two modes for locking in S3:
	- Governance Mode: specific IAM permissions are able to remove the lock.
	- Compliance Mode: lock protection cannot be removed by any user, including root.
- Two methods for locking in S3 Glacier:
	- Vault Access policy: can be modified.
	- Vault Lock policy: can be locked for compliance.
- S3 Object Legal Hold:
	- Prevents an object version from being overwritten or deleted.
	- Doesn't have an associated retention period and remains in effect until removed.
	- Legal holds can be freely placed and removed by any user who has the s3:PutObjectLegalHold permission. 


# S3 Monitoring options:
- CloudTrail Logs: 
	- provide a record of actions taken by a user, role, or an AWS service.
	- Data events incur a fee, in addition to storage of logs.
	- Speed of delivery: Data events every 5 mins; management events every 15 mins.
	- Log format: JSON.
- S3 Server Access Logs:
	- provide detailed records for the requests that are made to an S3 bucket or objects. 
	- Free.
	- Speed of delivery: Within a few hours.
	- Log format: unstructured log file.